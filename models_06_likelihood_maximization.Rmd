---
title: 'Model Fitting: Likelihood Maximization'
output:
  html_notebook: default
  html_document:
    df_print: paged
  pdf_document: default
author: Marius 't Hart
---

```{r setup, cache=FALSE, include=FALSE}
library(knitr)
opts_chunk$set(comment='', eval=FALSE)
```

Most model evaluation criteria don't work with minimizing a loss function (like MSE) but by maximizing the likelihood of the model. If you'd like to calculate a BIC, AICc or Hannan-Quinn criterion for your model - and probably others, you need the likelihood of the model.

_Note 1:_ This does not mean that your previous model fits are wrong. The likelihood of the model can be shown to be at it's maximum at the same parameters where the error is minimized. It's just that you can only use AIC for model evaluation.

_Note 2:_ I suppose it might be possible to fit a model using MSE minimization and then calculate the likelihood of the fitted model afterwards.

# Simple example of likelihood

This is going to be an extremely simple model. We're taking the sepal length of setosa irisses from the `iris` dataset:

```{r}
SSL <- iris$Sepal.Length[which(iris$Species == 'setosa')]
str(SSL)
```

Now, these 50 numbers can be described as a fairly normal distribution, as seen in histogram and QQ-plot:

```{r}
par(mfrow=c(1,2))
hist(SSL)
qqnorm(SSL, bty='n')
```

That means that the mean and standard deviation give a good description of the distribution:

```{r}
print(c('mean'=mean(SSL), 'sd'=sd(SSL)))
```

Our simple model has no other information than these two numbers. So if we ask it to predict what any new setosa's sepal length will be, the best answer is the mean: 5.006. But now, we can also ask how likely it is to observe this value, given what we know. For this we use the probability density function:

```{r}
plot(seq(3,7,0.001),dnorm(seq(3,7,0.001), mean=mean(SSL), sd=sd(SSL)), type='l',xlab='setosa sepal length', ylab='probability density', bty='n')
```

Notice that the curve exceeds 1. How can there be a probability larger than 1? That's because this is not a probability function, but a probability _density_ function. (It's impossible to have an actual probability function, since the chance of any exact value is infinitely small, best described as 0.) We could convert this to probabilities by slicing the curve into very tiny bins, and taking the surface of the bin, as the total surface is supposed to be 1. However, for our purposes we can apparently just use probability density, as it provides a _relative_ likelihood (within some range) of a particular value occurring as compared to other values.

Let's say that we only get the first ten of the 50 observations, and we can check the likelihood of those obervations relative to the whole dataset.

```{r}
print(data.frame('SSL'=SSL[1:10], 'likelihood'=dnorm(SSL[1:10], mean=mean(SSL), sd=sd(SSL))))
```

As you can see, closer to 5.006 the likelihood is highest.

## Two simple models

Now, we could make two simple models, one based on MSE minimization and based on likelihood maximization to find the best model. Both _should_ find the same solution. Let's see.

Here's a function that returns a mean square error:

```{r}
SSL_MSE <- function(par, data) {
  
  return( mean( (data-par)^2, na.rm=TRUE ) )
  
}
```

And a function that returs the likelihood:

```{r}
SSL_Likelihood <- function(par, data) {
  
  return( dnorm(par, mean=mean(data), sd=sd(data)) )
  
}
```

We can use `optim()` to find the best parameter estimate (`par`) for both, as `optim()` allows to set control$fncale to a negative value. A similar option is available for `optimx()`.

```{r}
control <- list()
control$fnscale <- -1
```

Here we fit it with the MSE:

```{r}
optim(par=4, fn=SSL_MSE, data=SSL, method='Brent', lower=0, upper=10)
```

And for the Likelihood:

```{r}
print(Lopt <- optim(par=4, fn=SSL_Likelihood, data=SSL, method='Brent', lower=0, upper=10, control=control))
```

## Evaluating the likelihood maximization model

So both methods find the exact same parameter values. Except for the likelihood we can now also calculate BIC and Hannan-Quinn:

```{r}

L <- Lopt$value
k <- 1
N <- 50

#-- AIC --#
AIC <- (2 * k) - (2 * log(L))

#-- BIC --#
BIC <- log(N)*k - (2 * log(L)) # L based
  
#-- Hannan-Quinn --#
HQC <- (-2 * L) + (2 * k * log(log(N))) # L based

print(c('AIC'=AIC, 'BIC'=BIC,'Hannan-Quinn'=HQC))
```

Of course, you could debate if those 50 observations are really independent.

## Take away

You should now have some sense of what a likelihood is and how you can maximize the likelihood of a model given the data.

# Two-Rate Model

And we will now implement this for a somewhat harder problem: the two-rate model. Here, there is a distribution for every trial. So we can optimize the likelihood if, in addition to the mean reach deviation in each trial, we also have the standard deviation of reach deviations on each trial.

Let's set up that part of the solution:

```{r}
load('data/tworatedata.rda')
baseline <- function(reachvector,blidx) reachvector - mean(reachvector[blidx], na.rm=TRUE)
tworatedata[,4:ncol(tworatedata)] <- apply(tworatedata[,4:ncol(tworatedata)], FUN=baseline, MARGIN=c(2), blidx=c(17:32))
mu <- apply(tworatedata[4:ncol(tworatedata)], 
            FUN=mean, 
            MARGIN=c(1), 
            na.rm=TRUE)
sigma <- apply(tworatedata[4:ncol(tworatedata)]-mu,
               FUN=sd,
               MARGIN=c(1),
               na.rm=TRUE )

reaches <- data.frame ( 'mean' = mu,
                        'sd'   = sigma )
schedule <- tworatedata$schedule


# This holds when the distribution is equally wide on every trial:
# sigma <- sd(as.matrix(tworatedata[4:ncol(tworatedata)] - mu), na.rm=T )
# reaches <- list( 'mean' = mu,
#                  'sd'   = sigma )
# Note that sd is not a single value.
```

Let's see what the data frame `reaches` looks like:

```{r}
str(reaches)
```

Our model function will still be the same:

```{r}
twoRateModel <- function(par, schedule) {
  
  # thse values should be zero at the start of the loop:
  Et <- 0 # previous error: none
  St <- 0 # state of the slow process: aligned
  Ft <- 0 # state of the fast process: aligned
  
  # we'll store what happens on each trial in these vectors:
  slow <- c()
  fast <- c()
  total <- c()
  
  # now we loop through the perturbations in the schedule:
  for (t in c(1:length(schedule))) {
    
    # first we calculate what the model does
    # this happens before we get visual feedback about potential errors
    St <- (par['Rs'] * St) - (par['Ls'] * Et)
    Ft <- (par['Rf'] * Ft) - (par['Lf'] * Et)
    Xt <- St + Ft
    
    # now we calculate what the previous error will be for the next trial:
    if (is.na(schedule[t])) {
      Et <- 0
    } else {
      Et <- Xt + schedule[t]
    }
    
    # at this point we save the states in our vectors:
    slow <- c(slow, St)
    fast <- c(fast, Ft)
    total <- c(total, Xt)
    
  }
  
  # after we loop through all trials, we return the model output:
  return(data.frame(slow,fast,total))
  
}
```


But the criterion function will be different. It should return a likelihood now, which is done in the last line of code and it is a somewhat complicate line. It can be done in a single line, because `dnorm()` takes vectorized input for any of its arguments (make sure that if two or more arguments are a vector, that they are of equal length, not sure what would happen if that is not the case).

Also, we now add some other criteria that return our lowLikelihood. These are to ensure that the returned model is "stable".

```{r}
twoRateLikelihood <- function(par, schedule, reaches) {
  
  lowLikelihood <- 0
  
  # learning and retention rates of the fast and slow process are constrained:
  if (par['Ls'] > par['Lf']) {
    return(lowLikelihood)
  }
  if (par['Rs'] < par['Rf']) {
    return(lowLikelihood)
  }
  
  # my own constraint:
  # if ((par['Ls']+par['Lf']) > 1) {
  #   return(lowLikelihood)
  # }
  
  # stability constraints:
  if ( ( ( (par['Rf'] - par['Lf']) * (par['Rs'] - par['Ls']) ) - (par['Lf'] * par['Ls'])) <= 0 ) {
    return(lowLikelihood)
  }
  
  p <- par['Rf']-par['Lf']-par['Rs']+par['Ls']
  q <- ( p^2 + (4 * par['Lf'] * par['Ls']) )
  if ( ((par['Rf'] - par['Lf'] + par['Rs'] - par['Ls']) + q^0.5) >= 2 ) {
    return(lowLikelihood)
  }

  # the following line works both if sd is a single value or a vector:
  return( prod( dnorm((twoRateModel(par, schedule)$total - reaches$mean), mean=0, sd=reaches$sd) ) )
  
} 
```

And now we can fit the model by maximizing it's likelihood, here simply with `optim()`:

```{r}
nvals <- 9
parvals <- seq(1/nvals/2,1-(1/nvals/2),1/nvals)

searchgrid <- expand.grid('Ls'=parvals,
                          'Lf'=parvals,
                          'Rs'=parvals,
                          'Rf'=parvals)

# evaluate starting positions:
Likelihoods <- apply(searchgrid, FUN=twoRateLikelihood, MARGIN=c(1), schedule=schedule, reaches=reaches)

# we need to tell optimx to maximize the likelihood:
control <- list()
control$maximize <- TRUE

# run optimx on the best starting positions:
allfits <- do.call("rbind",
                   apply( searchgrid[order(Likelihoods, decreasing = T)[1:10],],
                          MARGIN=c(1),
                          FUN=optimx,
                          fn=twoRateLikelihood,
                          method='L-BFGS-B',
                          lower=c(0,0,0,0),
                          upper=c(1,1,1,1),
                          schedule=schedule,
                          reaches=reaches,
                          control=control ) )

# pick the best fit:
win <- allfits[order(allfits$value, decreasing = T)[1],]
print(par <- unlist(win[,1:4]))
```

Let's see the quality of the fit:

```{r}
model <- twoRateModel(par=par, schedule=schedule)
plot(reaches$mean,type='l',col='#333333',xlab='trial',ylab='reach deviation [deg]',xlim=c(0,165),ylim=c(-35,35),bty='n',ax=F)
lines(c(1,33,33,133,133,145,145),c(0,0,30,30,-30,-30,0),col='#AAAAAA')
lines(c(145,164),c(0,0),col='#AAAAAA',lty=2)
lines(model$slow,col='blue')
lines(model$fast,col='red')
lines(model$total,col='purple')
axis(1,c(1,32,132,144,164),las=2)
axis(2,c(-30,-15,0,15,30))
```

Why does that look worse than MSE?

```{r}
MSEpar <- c('Ls'=0.07296962,
            'Lf'=0.42197434,
            'Rs'=0.99858527,
            'Rf'=0.68592189)

MSE_likelihood <- twoRateLikelihood(par=MSEpar, schedule=schedule, reaches=reaches)
LIK_likelihood <- twoRateLikelihood(par=par, schedule=schedule, reaches=reaches)
print(likelihoods <- c('MSE'=MSE_likelihood, 'L'=LIK_likelihood))

```

The likelihood is actually lower for the maximum likelihood algorithm?

It's higher now that I've included some of Thomas' stability constraints, but still not the same. Why?

# Double-Drift Reset Points

Another possibility is that our likelihood function takes some irregular shape that is not described by an existing function, or when it occurs in multiple dimensions. This is the case for the Double-Drift Reset Points data that we covered in the Model Evaluation tutorial. At least, the data is best described as a two-dimensional distribution, which might have an irregular shape. I'll try to solve that here.

It could be seen as the product of two likelihood functions, but we might have to rotate the data with PCA first, so that the two likelihood functions operate on one axis only (or... mostly). 