---
title: 'Model Fitting'
output:
  html_notebook: default
  html_document:
    df_print: paged
  pdf_document: default
author: Marius 't Hart
---

```{r setup, cache=FALSE, include=FALSE}
library(knitr)
opts_chunk$set(comment='', eval=FALSE)
```

Computational models provide a (mathematically) precise explanation for observed data. Some people will say that all models are wrong, but then again, some models are more wrong than others. Here you will learn how to build and fit some simple models using optimization techniques.

You'll start with a relatively simple example. In some of our experiments we ask people to localize where they think their hand is. Recently, we've been asking them to indicate this on a visual arc that is projected just above their actual hand. This requires the projection to be aligned with the hand, but unfortunately, this was not always the case.

Let's load some data, and see how bad it is.

```{r}
load('data/localization.rda')
```

This loads two data frames. Both have 5 columns, first an angle the experiment was trying to put the hand at (`targetangle_deg`), then the recorded x and y coordinates of where the hand ended up (in centimeters: `handx_cm` and `handy_cm`) and where the participant indicated they thought their hand was as they indicated by tapping on a touch screen (`tapx_cm` and `tapy_cm`). This should all be centered on the 0,0 position. For now, the focus is on how accurate the measurements were, not how accurate the responses were, so we only plot the taps:


```{r}
par(mfrow=c(1,2))
for (dfno in c(1,2)) {
  df <- list(localization,mislocalization)[[dfno]]
  plot( c(0,cos(seq(0,pi/2,(pi/2)/100))*10, 0),
        c(0,sin(seq(0,pi/2,(pi/2)/100))*10, 0),
        type='l', col='#AAAAAA', 
        main=c('aligned','misaligned')[dfno], xlab='x [cm]', ylab='y [cm]', 
        xlim=c(-2,12), ylim=c(-2,12),
        bty='n', ax=F, asp=1)
  points(df$tapx_cm,df$tapy_cm)
  axis(side=1,at=c(0,5,10))
  axis(side=2,at=c(0,5,10))
}
```

For the `misaligned` data frame it looks like the measurements were off by perhaps two centimeter: everything is shifted upwards. However it still falls on a circle, so we can figure out how misaligned the measurements really were.

You'll build a fairly simple model to do this. The type of model fitting we do involves minimize the (mean) **squared** error between the data and the model's output (given some input parameters). The first thing to do is write a function that calculates how wrong the model is (remember, all models are wrong). The second step is finding the parameters values that minimize this error. The second step can be done with functions that are built into R, but the first step really requires thinking about what is going on in your data. (This, by itself, is already useful.)

First, all points falling on a circle will have the same distance to the centre of the circle, and this is the radius. In this case the radius is 10 centimeters.

```{r}
circleFitError <- function(par, coords, radius){
  
  return(mean((sqrt((coords$x - par['x'])^2 + (coords$y - par['y'])^2) - radius)^2, na.rm=TRUE))
  
}
```


This function takes three arguments: `par` is a named, numeric vector with an `x` and `y` coordinate of the centre of the circle, `coords`, which is a data frame with an `x` and `y` column of localization positions, and `radius` which is the radius of the circle the data should fall on.

The first argument to these error functions in model fitting always specify parameters that we want to fit. We could also fit the radius of the circle, but it is better to keep your model as simple as possible, and in this case, we know that the radius is not affected by shifting the visual stimulus. The other arguments allow specifying data and other parameters that you don't want to fit.

We can try this function on both data sets, and the error in the misaligned data set should be larger than in the aligned dataset.

```{r}
par <- c('x'=0,'y'=0) # this would be the centre of the circle given perfect measurements
loccoords <- data.frame('x'=localization$tapx_cm, 'y'=localization$tapy_cm)
miscoords <- data.frame('x'=mislocalization$tapx_cm, 'y'=mislocalization$tapy_cm)

circleFitError(par=par,coords=loccoords,radius=10)
circleFitError(par=par,coords=miscoords,radius=10)

```

The `aligned` data set indeed has smaller errors than the `misaligned` data set. The square root of the error in the `misaligned` data set is ~1.6, so this is how far the average localization point's distance to (0,0) is different from 10 cm. Notice that this does not indicate how far the centre of the measurements is away from the origin. Some of the data is pretty close to a circle with radius of 10 cm; those to the right, so their errors will be close to 0, while the points at the top are further away.

# Optim

Base R has a function that allows fitting such models to data: `optim()`. We will explain it's use here, but realize that it is considered deprecated by it's author (John Nash) and that he recommends using the packages `optimr` or `optimx` instead. We will discuss `optimx` later.

The function `optim()` allows several fitting algorithms, where the default, and simplest option, is Nelder-Mead. You'll explore this here first. For many simple problems, this is also good enough, but for more complicated problems you should use more modern approaches.

The syntax for `optim()` requires a set of starting parameter values in it's `par` argument, and an error function in it's `fn` argument. We already have this error function. The `...` argument allows us to specify arguments to the error function. The other arguments are interesting, but here we'll first see how this approach to fitting models works. Let's try finding the centre for the localization data in the `mislocalization` data frame:

```{r}
optim( par = c( 'x'=0,
                'y'=0),
       fn = circleFitError,
       coords = miscoords,
       radius = 10)
```

This returns a named list. The `$par` element gives you the fitted parameters it found, in this case (-.19, 1.99), so about 2 centimeters up, and little bit to the left. The `$value` element provides the value the error function returns with the fitted parameters. This value (0.447) is much lower than the one we got before (it was 2.57 with x=0, y=0) so this looks like an improvement. The `$counts` element tells you how many times `optim()` ran the error function to try out different parameters. That was 65 times, so luckily you didn't have to do this by hand.

Let's see how well it does with the aligned data:

```{r}
optim( par = c( 'x'=15,
                'y'=15),
       fn = circleFitError,
       coords = loccoords,
       radius = 10)
```

The error value is larger than it was before fitting and `optim()` returned a location for the centre of the circle at x = 11.5 cm and y = 14.2 cm. What went wrong here? Fix the code!

# Optimx

So you saw that `optim()` is really nice and easy to use, but we also know that it is considered deprecated. So we _should_ be using better methods. The package `optimx` allows this, as it supercedes many existing optimization packages and even allows you to point to new optimization functions or packages.

There is something to be said for using `optim()` though: it comes with base R. That means that if you use R Markdown notebooks to publish your analyses, it doesn't require to be installed, so it is a good fall-back method. I have not done this consistently yet, but it could be a good idea to test if `optimx` is installed and if it is, your code can use it, and if not, the code can use `optim()` instead.

Run this code to see if `optimx` is installed on your system:

```{r}
if ('optimx' %in% installed.packages()) {
  cat('optimx is installed, you can continue with the next exercise\n')
} else {
  cat('optimx is NOT installed, please install it before doing the next exercise\n')
}
```

If you just want to use the methods that `optim()` offers, that is simple, as `optimx` is only a front-end to other optimization packges, including `optim()`.

Let's see if we use the Nelder-Mead method from `optim()` by calling the function `optimx()` from the `optimx` package. First, we load the package, and then ask for help on the function:

```{r}
library(optimx)
help(optimx)
```

That looks pretty similar to how `optim()` is used, so let's try it. Complete this chunk to get it to work, compare to the calls to `optim()` above:

```{r}
optimx( par = c( 'x'=0,
                 'y'=0),
       fn = circleFitError,
       coords = miscoords)
```

Wow, this output looks different. It seems to allow different methods at the same time, and provides information on all different methods in a nicer looking table (or data frame?). In the help text for `optimx()` we saw the two default functions, to be Nelder-Mead and BFGS, and apparently, if we don't specify the methods, it does all of them. That is cool, but isn't one method enough?

Let's try how well this BFGS method does with the incorrect starting parameters for the "good" localization data set:

```{r}
optimx( par = c( 'x'=15,
                 'y'=15),
       fn = circleFitError,
       coords = loccoords,
       radius = 10)
```

It does about equally bad, so we probably need to get a different solution for this problem.

# Benchmarking

For this data it doesn't matter so much as the model is fitted very quickly, but once you start getting into more complicated problems, you might want to think about optimizing your code a little bit. One way to do this for model fitting is to see which method runs faster. In the tables from `optimx()` we already got a clue. The column `fevals` tells us how often the error function was evaluated with each of the two methods. Nelder-Mead needed to use the error function 55 times, whereas BFGS only used 10 times. However, there might be other factors at play that affect the total time used for a single model fit.

There are ways to measure the total time it takes to run any command. If you're not interested, this section can be skipped.

We can use the `microbenchmark` package to do this. Specifically, it allows several commands to be tested at the same time, and provides some very nice output, even pretty plots if you want them. We need to specify the commands to test and how many times to test them (for more accurate results):

```{r}
library(microbenchmark)
res <- microbenchmark('Nelder-Mead' = optimx( par = c( 'x'=1, 'y'=1), 
                                              fn = circleFitError, 
                                              method='Nelder-Mead', 
                                              coords = loccoords, radius = 10),
                      'BFGS' = optimx( par = c( 'x'=1, 'y'=1), 
                                       fn = circleFitError, 
                                       method='BFGS', 
                                       coords = loccoords, radius = 10),
                      times = 100
               )
print(res)
```


As promised, this allows easy plots:

```{r}
boxplot(res)
```

As well as pretty ggplot2 visualization:

```{r}
if (require("ggplot2")) {
  autoplot(res)
}

```

It doesn't really look like there is much of a difference between the two methods though.

# `L-BFGS-B` and search boundaries

Sometimes the model we specify can only accept parameters in a specific range. That is, the model can't be evaluated if the parameters are outside of the range, or perhaps we already know that parameters outside this range don't make sense, even when the model can be fit with them (negative radius of a circle?). Either way, some optimization methods allow restricting the parameter search space, which may result in faster search, or better results. You will investigate one of those here. We will also benchmark this method on the mislocalization data set, as we don't really need it for the other data set anyway.

```{r}
res <- microbenchmark('Nelder-Mead' = optimx( par = c( 'x'=0.1, 'y'=0.1), 
                                              fn = circleFitError, 
                                              method='Nelder-Mead', 
                                              coords = miscoords, radius = 10),
                      'BFGS' = optimx( par = c( 'x'=0.1, 'y'=0.1), 
                                       fn = circleFitError, 
                                       method='BFGS', 
                                       coords = miscoords, radius = 10),
                      'L-BFGS-B' = optimx( par = c( 'x'=0.1, 'y'=0.1), 
                                           fn = circleFitError, 
                                           lower = c(-5,-5), upper=c(5,5), # boundaries!
                                           method='L-BFGS-B', 
                                           coords = miscoords, radius = 10),
                      times = 100
               )
print(res)
```

Still doesn't matter much in this example, but at least you figured out how to use parameter search boundaries!

# Control parameters

Both in calls to `optim()` and `optimx()` you can specify a set of controls, that fine-tune how the search through the parameter space is done, or the criteria to stop it. Most of the time, the defaults are OK, but here we'll look into a few of the values we can change there.

## `fnscale`

## `ndeps`

## `maxit`, `abstol` and `reltol` (or `pgtol`)



# Local minima

As you saw when fitting the model with very incorrect starting parameters, sometimes the algorithm doesn't converge on a good estimate. A good choice of starting parameters is therefore also important. For the circle fitting problem, you could say that the average of the x-coordinates that are in the data and the average of the y-coordinates that are in the data, would always avoid a wrong set of starting parameters:

```{r}
optim( par = c( 'x'=mean(loccoords$x),
                'y'=mean(loccoords$y)),
       fn = circleFitError,
       coords = loccoords,
       radius = 10)
```

So this means that for some problems, you can usually (or always?) make a reasonable guess as to what the true parameters should be, or at least put them somewhere in the search space, where a search algorithm doesn't get stuck on an incorrect location.

Such incorrect locations are called 'local minima'. The presence of local minima is not always obvious or detectable, especially with models where the parameters interact in sometimes unpredictable ways. One way to get out of this is to do a _"grid search"_ first.

## Grid search

In grid search, you evaluate the error function on a set of parameter values that covers the space of potential parameter values. Then you take a few of the parameter combinations with a low error, say the five parameters with the lowest errors. You run the fitting algorithm on those and record the parameters with the lowest error as the fit that you will use.

We'll do a simplified example of this here, still using the circle data. It is called _grid_ search, as you're supposed to make a grid of all the combinations of a list of possible parameter values. In this case, we use a few starting positions that are in the range of the data.

```{r}
xvals <- seq(min(miscoords$x),max(miscoords$x),diff(range(miscoords$x))/3) # 4 x-values
yvals <- seq(min(miscoords$y),max(miscoords$y),diff(range(miscoords$y))/3) # 4 y-values
searchgrid <- expand.grid('x'=xvals, 'y'=yvals) # all 16 combinations
kable(searchgrid)
```

Now we can use apply on each row of that data frame to get the mean squared errors for each of the combinations of x and y coordinates:

```{r}
MSE <- apply(searchgrid,FUN=circleFitError,MARGIN=c(1),coords=miscoords,radius=10)
print(MSE)
```

We can now use the lowest 3 MSE's as starting points:

```{r}
topgrid <- searchgrid[order(MSE)[1:3],]
print(topgrid)
```

And basically we'll apply `optimx()` to each row of that data frame:

```{r}
allfits <- do.call("rbind",
                   apply( topgrid,
                          MARGIN=c(1),
                          FUN=optimx,
                          fn=circleFitError,
                          method='Nelder-Mead',
                          coords=miscoords,
                          radius=10 ) )
kable(allfits)
```

The second row shows a local minimum, but the first and third row are pretty good (perhaps equally good), and with very similar x and y coordinates. However, we need to pick one, and it makes sense to pick the row with the lowest number in the column `value`:

```{r}
winningmodel <- allfits[order(allfits$value)[1],]
print(winningmodel)
```

If there is a tie, you might use the number of function evaluations to break it (lower is better?) but that should be rare. In essence we get the same fitted circle centre as we got several times earlier. Let's see how well that circle works on all this data, integrating all we have done so far:

```{r}
par(mfrow=c(1,2))
for (dfno in c(1,2)) {
  # get the right data frame:
  df <- list(loccoords,miscoords)[[dfno]]
  # plot an ideal quarter circle:
  plot( c(0,cos(seq(0,pi/2,(pi/2)/100))*10,0), 
        c(0,sin(seq(0,pi/2,(pi/2)/100))*10,0), 
        type='l', col='#AAAAAA', 
        main=c('aligned','misaligned')[dfno], xlab='x [cm]', ylab='y [cm]', 
        xlim=c(-2,12),ylim=c(-2,12), 
        bty='n', ax=F, asp=1)
  # scatter plot of the actual data:
  points(df$x,df$y)
  # make search grid:
  searchgrid <- expand.grid('x'=seq(min(df$x),max(df$x),diff(range(df$x))/3),
                            'y'=seq(min(df$y),max(df$y),diff(range(df$y))/3)) 
  # evaluate starting positions:
  MSE <- apply(searchgrid,FUN=circleFitError,MARGIN=c(1),coords=df,radius=10)
  # run optimx on the best starting positions:
  allfits <- do.call("rbind",
                   apply( searchgrid[order(MSE)[1:3],],
                          MARGIN=c(1),
                          FUN=optimx,
                          fn=circleFitError,
                          method='Nelder-Mead',
                          coords=df,
                          radius=10 ) )
  # pick the best fit:
  win <- allfits[order(allfits$value)[1],]
  # plot the centre:
  points(win$x,win$y,col='red')
  # plot a circle of radius 10 around the centre:
  lines( (cos(seq(0,pi/2,(pi/2)/100))*10) + win$x, 
         (sin(seq(0,pi/2,(pi/2)/100))*10) + win$y, col='red')
  axis(side=1,at=c(0,5,10))
  axis(side=2,at=c(0,5,10))
}
```

## Local minima

Let's back-track a bit to see why grid search can help us circumvent local minima. First, we'll visualize the grid search for this circle fit. We'll use a slightly denser search grid, and notice that it extends well beyond the previous area. This is to show the overal landscape of fit quality a little better.

```{r}
searchgrid <- expand.grid('x'=seq(-8,22,1), 'y'=seq(-8,22,1))
MSE <- apply(searchgrid,FUN=circleFitError,MARGIN=c(1),coords=miscoords,radius=10)
```

Now we can plot the size of the error over the x,y coordinates of where that error came from as a mesh:

```{r}
persp(unique(searchgrid$x), unique(searchgrid$y), matrix(sqrt(MSE), ncol=length(unique(searchgrid$x))), phi = 60, theta = 35,
  xlab = "X-coordinate", ylab = "Y-coordinate", zlab='MSE',
  main = "searchgrid MSEs", col='#999999', shade=0.7, border=NA
)
```

Notice how the X and Y axis are directed, as indicate by the arrows (can be hard to see).

You can now see that the MSE doesn't vary randomly throughout the search space. For the most part, there is a valley with an optimal lowest point: the further you get away from this point, the larger the error. There is a little ridge of higher errors, this is where the data is. You can also see that if you put the centre of the circle at a point to the top/right of that ridge, there is a low point too. Whichever way you move from there, the errors go up, and that is the additional local minimum in the parameter space around (~11,~16).

Here I show the lowest MSEs on top of a simpler landscape:

```{r}
searchgrid <- expand.grid('x'=seq(-4,18,2), 'y'=seq(-4,18,2))
MSE <- apply(searchgrid,FUN=circleFitError,MARGIN=c(1),coords=miscoords,radius=10)
persp(unique(searchgrid$x), unique(searchgrid$y), matrix(sqrt(MSE), ncol=length(unique(searchgrid$x)), byrow=FALSE), phi = 60, theta = 40,
  xlab = "X-coordinate", ylab = "Y-coordinate", zlab='MSE',
  main = "searchgrid MSEs", col='#999999', shade=0.7, border=NA
) -> res
bestidx <- order(MSE)[1:8]
best <- searchgrid[bestidx,]
points(trans3d(best[,1],best[,2],sqrt(MSE[bestidx]), pmat=res), col='red')
```

When you start the search through parameter space at that one point at the local minimum, your search will not find the best fit, since any small change in fit parameters from that point will _increase_ the MSE. This 2-dimensional example allows you to see this, but for most optimization problems it isn't so easy to see, as usually there are more parameters.

# Two-rate model of visuomotor adaptation

You are now ready to implement a more interesting model, let's implement a simple version of the two-rate model of motor learning... or of visuomotor adaptation, as the case may be.

As you know, this model assumes that the reach deviation X on a trial t is the sum of the output of a fast process F and a slow process S also on trial t:

$X_t = F_t + S_t$

And each of these processes learn from the error E on the previous trial and retain some of their activity F or S from the previous trial:

$F_t = R_f \cdot F_{t-1} + L_f \cdot E_{t-1}$

and

$S_t = R_s \cdot S_{t-1} + L_s \cdot E_{t-1}$

As is customary, this model is fitted on data where there is an error clamp phase, where errors are set to zero. In those phases, the learning has no effect, as the errors are 0, but the retention factors fully determine the reach output.

First, we need some data, let's load that:

```{r}
load('data/tworatedata.rda')
```

In order to fit this model, we need to have one set of reaches, let's see the average or median across the participants. We'll want to make the model flexible, so that it can fit any kind of paradigm. This means we also need to have some way to tell the model what the paradigm is.

Before we can get the reach deviations, we need to baseline the reaches for every participant, by subtracting the median reach deviations from the aligned phase from all reach deviations:

```{r}
baseline <- function(reachvector,blidx) reachvector - mean(reachvector[blidx], na.rm=TRUE)
tworatedata[,4:ncol(tworatedata)] <- apply(tworatedata[,4:ncol(tworatedata)], FUN=baseline, MARGIN=c(2), blidx=c(17:32))
```

Now we'll get the median reach deviations across participants, and plot them.

```{r}
reaches <- apply(tworatedata[4:ncol(tworatedata)], FUN=median, MARGIN=c(1), na.rm=TRUE)
plot(reaches,type='p',xlab='trial',ylab='reach deviation [deg]',xlim=c(0,165),ylim=c(-35,35),bty='n',ax=F)
lines(c(1,33,33,133,133,145,145),c(0,0,30,30,-30,-30,0),col='#AAAAAA')
lines(c(145,164),c(0,0),col='#AAAAAA',lty=2)
axis(1,c(1,32,132,144,164),las=2)
axis(2,c(-30,-15,0,15,30))
```

OK, so that looks like pretty decent data, with a spontaneous rebound that we can possibly explain with the two-rate model. For this model we will write two separate functions to get the mean squared error. The first needs a schedule and set of parameter values to run the model and calculate the model's reach deviations. The second needs a schedule, a set of parameter values and some actual reach data. It checks the sanity of the parameters, then uses the first function and returns the MSE - or some "large" error value if some of the model's constraints are not met.

## Model function

The function that runs the model, will use a for loop. I'm not sure exactly why, but smart people tell me this can't be solved in one line.

```{r}
twoRateModel <- function(par, schedule) {
  
  # thse values should be zero at the start of the loop:
  Et <- 0 # previous error: none
  St <- 0 # state of the slow process: aligned
  Ft <- 0 # state of the fast process: aligned
  
  # we'll store what happens on each trial in these vectors:
  slow <- c()
  fast <- c()
  total <- c()
  
  # now we loop through the perturbations in the schedule:
  for (t in c(1:length(schedule))) {
    
    # first we calculate what the model does
    # this happens before we get visual feedback about potential errors
    St <- (par['Rs'] * St) - (par['Ls'] * Et)
    Ft <- (par['Rf'] * Ft) - (par['Lf'] * Et)
    Xt <- St + Ft
    
    # now we calculate what the previous error will be for the next trial:
    if (is.na(schedule[t])) {
      Et <- 0
    } else {
      Et <- Xt + schedule[t]
    }
    
    # at this point we save the states in our vectors:
    slow <- c(slow, St)
    fast <- c(fast, Ft)
    total <- c(total, Xt)
    
  }
  
  # after we loop through all trials, we return the model output:
  return(data.frame(slow,fast,total))
  
}
```

We can play around with this model, by giving it some more or less common parameter values:

```{r}
par <- c('Ls'=.05, 'Lf'=.15, 'Rs'=.99, 'Rf'=.75)
schedule <- tworatedata$schedule
model <- twoRateModel(par=par, schedule=schedule)
```

That seems to work reasonably well, lets add this to our plot of the data:

```{r}
plot(reaches,type='l',col='#333333',xlab='trial',ylab='reach deviation [deg]',xlim=c(0,165),ylim=c(-35,35),bty='n',ax=F)
lines(c(1,33,33,133,133,145,145),c(0,0,30,30,-30,-30,0),col='#AAAAAA')
lines(c(145,164),c(0,0),col='#AAAAAA',lty=2)
lines(model$slow,col='blue')
lines(model$fast,col='red')
lines(model$total,col='purple')
axis(1,c(1,32,132,144,164),las=2)
axis(2,c(-30,-15,0,15,30))
```

These parameters undershoot the learning in the initial phase, as well as the rebound. How do we find better parameter values? We could play around with the settings until we find some values that work well, but that could take a long time and there is no guarantee that those values would be even close to the best parameters. Instead we want to have the computer do the heavy work, by searching the parameter space in a systematic way.

Now that we have a function that runs the model, we can use that in a function that calculates how wrong the model parameters are. We can calculate the mean of the squared difference between the total model output and the actual reach deviations that we measured. So we will definitely do that.

## Error function [1/2]

This model can be wrong in other ways too. First, all parameters have to be in between 0 and 1 (perhaps they can also be 0 or 1), second, the slow learning rate should be lower than the fast learning rate and third, the slow retention rate should be larger than the fast retention rate. 

```{r}
twoRateMSE <- function(par, schedule, reaches) {
  
  # parameter values should be between 0 and 1:
  if (any(par > 1)) {
    return(Inf)
  }
  if (any(par < 0)) {
    return(Inf)
  }
  
  # learning and retention rates of the fast and slow process are constrained:
  if (par['Ls'] > par['Lf']) {
    return(Inf)
  }
  if (par['Rs'] < par['Rf']) {
    return(Inf)
  }
  
  return( mean((twoRateModel(par, schedule)$total - reaches)^2, na.rm=TRUE) )
  
} 
```

For the parameters that we just came up with, the MSE would be:

```{r}
print(twoRateMSE(par, schedule, reaches))
```

That is on average 5 degrees of on every trial, including the aligned phase. This can be improved!

## First optimization

The strategy from before was to use `optimx()` so let's see if that works:

```{r}
library(optimx)
optimx(par = par, 
       fn = twoRateMSE,
       schedule = schedule,
       reaches = reaches)
```

First of all, BFGS doesn't return anything, and while Nelder-Mead reduces the MSE to 5.2 (so ~2.3 degrees error on every trial), there is a warning about eigenvalue failures and it still takes 277 function evaluations.

## Error function [2/2]

There are some problems with the above function. First, returning `Inf` as a very large error might be too much. Let's change this to 10 times the MSE if the model learned nothing at all. Second, we built in the boundaries in the error function, but we can set them explicitly, at least for some of the fitting methods. Let's try to fix both. First, we change the error function:

```{r}
twoRateMSE <- function(par, schedule, reaches) {
  
  bigError <- mean(schedule^2, na.rm=TRUE) * 10
  
  # learning and retention rates of the fast and slow process are constrained:
  if (par['Ls'] > par['Lf']) {
    return(bigError)
  }
  if (par['Rs'] < par['Rf']) {
    return(bigError)
  }
  
  return( mean((twoRateModel(par, schedule)$total - reaches)^2, na.rm=TRUE) )
  
} 
```

This is what `optimx()` does with this:

```{r}
library(optimx)
optimx(par = par, 
       fn = twoRateMSE,
       schedule = schedule,
       reaches = reaches)
```

Now both methods work and seem to provide similar model fits. However, we still want to set the upper and lower bounds in our call to `optimx()`:

```{r}
library(optimx)
optimx(par = par, 
       fn = twoRateMSE,
       lower = c(0,0,0,0),
       upper = c(1,1,1,1),
       schedule = schedule,
       reaches = reaches)
```

That works, but doesn't seem to change all that much.

## Benchmarking optimization methods

Now for this model it might be more interesting to see how fast each of the methods works:

```{r}
library(microbenchmark)
res <- microbenchmark('Nelder-Mead' = optimx( par = par, 
                                              fn = twoRateMSE, 
                                              method='Nelder-Mead', 
                                              schedule = schedule, reaches = reaches),
                      'BFGS' = optimx( par = par, 
                                       fn = twoRateMSE, 
                                       method='BFGS', 
                                       schedule = schedule, reaches = reaches),
                      'L-BFGS-B' = optimx( par = par, 
                                           fn = twoRateMSE, 
                                           lower = c(0,0,0,0), upper=c(1,1,1,1), # boundaries!
                                           method='L-BFGS-B', 
                                           schedule = schedule, reaches = reaches),
                      times = 20
               )

print(res)

if (require("ggplot2")) {
  autoplot(res)
}
```

Nelder-Mead might be slightly faster in this case, despite the warning from the author of `optim` that it is quite slow. However, our function now doesn't check boundaries by itself anymore, so we are going to go with `L-BFGS-B` (whatever that stands for).

## Two-rate grid search

At this point, we can run our optimization algorithm of choice on our model so that it fits the data, but there is one possible problem that we still want to take care of: local minima. We'll use the same solution as we had above: grid search!

```{r}
nvals <- 5
parvals <- seq(1/nvals/2,1-(1/nvals/2),1/nvals)

searchgrid <- expand.grid('Ls'=parvals,
                          'Lf'=parvals,
                          'Rs'=parvals,
                          'Rf'=parvals)
# evaluate starting positions:
MSE <- apply(searchgrid, FUN=twoRateMSE, MARGIN=c(1), schedule=schedule, reaches=reaches)
# run optimx on the best starting positions:
allfits <- do.call("rbind",
                   apply( searchgrid[order(MSE)[1:10],],
                          MARGIN=c(1),
                          FUN=optimx,
                          fn=twoRateMSE,
                          method='L-BFGS-B',
                          lower=c(0,0,0,0),
                          upper=c(1,1,1,1),
                          schedule=schedule,
                          reaches=reaches ) )
# pick the best fit:
win <- allfits[order(allfits$value)[1],]
print(win[1:4])
```

The MSE `value` is very similar to the one we had before without grid search (5.286). Perhaps this means we didn't need grid search after all, but with this model there is a 4-dimensional parameter space and no obvious way to find where local minima are, so it is better to do grid search.

## Plot model

Let's plot the winning model on top of the data:

```{r}
model <- twoRateModel(par=unlist(win[,c(1:4)]), schedule=schedule)
plot(reaches,type='l',col='#333333',xlab='trial',ylab='reach deviation [deg]',xlim=c(0,165),ylim=c(-35,35),bty='n',ax=F)
lines(c(1,33,33,133,133,145,145),c(0,0,30,30,-30,-30,0),col='#AAAAAA')
lines(c(145,164),c(0,0),col='#AAAAAA',lty=2)
lines(model$slow,col='blue')
lines(model$fast,col='red')
lines(model$total,col='purple')
axis(1,c(1,32,132,144,164),las=2)
axis(2,c(-30,-15,0,15,30))
```

That _does_ look a lot better than with the parameters we made up.

# Model quality

Usually, what we will want to do is test how well one given model explains data from two different conditions or test how well two different models explain the same data. One of the assumptions there is that both model fits are really optimal model fits, so taking all the extra steps described above makes sense. Evaluating the quality of the model fits is a statistics question, so the goal is to add a tutorial to the statistics section about this issue.